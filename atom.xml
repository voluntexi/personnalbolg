<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://voluntexi.github.io/</id>
    <title>威伦特</title>
    <updated>2024-08-20T08:21:42.675Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://voluntexi.github.io/"/>
    <link rel="self" href="https://voluntexi.github.io/atom.xml"/>
    <subtitle>解码生命</subtitle>
    <logo>https://voluntexi.github.io/images/avatar.png</logo>
    <icon>https://voluntexi.github.io/favicon.ico</icon>
    <rights>All rights reserved 2024, 威伦特</rights>
    <entry>
        <title type="html"><![CDATA[正确使用 ORDER BY + LIMIT]]></title>
        <id>https://voluntexi.github.io/zheng-que-shi-yong-order-by-limit/</id>
        <link href="https://voluntexi.github.io/zheng-que-shi-yong-order-by-limit/">
        </link>
        <updated>2024-08-20T07:54:21.000Z</updated>
        <summary type="html"><![CDATA[<p>当我们想在分页查询中对数据进行排序展示时，通常会使用 <code>ORDER BY</code> 进行排序。然而，当用于排序的字段并非唯一时，可能会在翻页时遇到数据重复的问题，下面是对这个问题的具体分析和解决方案。</p>
]]></summary>
        <content type="html"><![CDATA[<p>当我们想在分页查询中对数据进行排序展示时，通常会使用 <code>ORDER BY</code> 进行排序。然而，当用于排序的字段并非唯一时，可能会在翻页时遇到数据重复的问题，下面是对这个问题的具体分析和解决方案。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prompt Learning]]></title>
        <id>https://voluntexi.github.io/prompt-learning/</id>
        <link href="https://voluntexi.github.io/prompt-learning/">
        </link>
        <updated>2023-12-12T14:01:05.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Prompt Learning 的本质就是将所有下游任务统一成预训练任务；</strong> 以特定的模板，将下游任务的数据转成自然语言形式，从而充分挖掘预训练语言模型本身的能力。</p>
]]></summary>
        <content type="html"><![CDATA[<p><strong>Prompt Learning 的本质就是将所有下游任务统一成预训练任务；</strong> 以特定的模板，将下游任务的数据转成自然语言形式，从而充分挖掘预训练语言模型本身的能力。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More for Long Document Summary Evaluation by LLMs]]></title>
        <id>https://voluntexi.github.io/less-is-more/</id>
        <link href="https://voluntexi.github.io/less-is-more/">
        </link>
        <updated>2023-10-09T08:51:46.000Z</updated>
        <summary type="html"><![CDATA[<p>这篇文章给了我们一种如何在自己研究的领域去&quot;蹭&quot;大模型热度的思路</p>
]]></summary>
        <content type="html"><![CDATA[<p>这篇文章给了我们一种如何在自己研究的领域去&quot;蹭&quot;大模型热度的思路</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating EDU Extracts for Plan-Guided Summary Re-Ranking]]></title>
        <id>https://voluntexi.github.io/PGA/</id>
        <link href="https://voluntexi.github.io/PGA/">
        </link>
        <updated>2023-09-09T08:21:25.000Z</updated>
        <summary type="html"><![CDATA[<p>这篇文章是在我之前介绍的<strong>BRIO模型（<a href="https://voluntexi.github.io/brio/">BRIO | 威伦特 (voluntexi.github.io)</a>）<strong>的基础上改进的，模型的整体框架也是采用两步式摘要，即结合</strong>生成候选摘要</strong>和<strong>评估候选摘要</strong>两个阶段来获得最佳摘要。</p>
]]></summary>
        <content type="html"><![CDATA[<p>这篇文章是在我之前介绍的<strong>BRIO模型（<a href="https://voluntexi.github.io/brio/">BRIO | 威伦特 (voluntexi.github.io)</a>）<strong>的基础上改进的，模型的整体框架也是采用两步式摘要，即结合</strong>生成候选摘要</strong>和<strong>评估候选摘要</strong>两个阶段来获得最佳摘要。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copy is All You Need]]></title>
        <id>https://voluntexi.github.io/copy-is-all-you-need/</id>
        <link href="https://voluntexi.github.io/copy-is-all-you-need/">
        </link>
        <updated>2023-08-21T09:48:54.000Z</updated>
        <summary type="html"><![CDATA[<p>最近在<a href="https://paperswithcode.com/">paper with code</a>刷论文的时候，看到了一个很唬人的文章“《Copy is All You Need》”，遂找来研读研读，发现内容还是很有意思，准备写一篇阅读笔记的，偶然发现了这篇文章作者的采访稿，将文章背后的故事都介绍的挺详细的。于是乎转载一下（不是偷懒）</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近在<a href="https://paperswithcode.com/">paper with code</a>刷论文的时候，看到了一个很唬人的文章“《Copy is All You Need》”，遂找来研读研读，发现内容还是很有意思，准备写一篇阅读笔记的，偶然发现了这篇文章作者的采访稿，将文章背后的故事都介绍的挺详细的。于是乎转载一下（不是偷懒）</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ LONGNET: Scaling Transformers to 1,000,000,000 Tokens]]></title>
        <id>https://voluntexi.github.io/longnet/</id>
        <link href="https://voluntexi.github.io/longnet/">
        </link>
        <updated>2023-07-21T11:32:15.000Z</updated>
        <summary type="html"><![CDATA[<p>前段时间刚介绍了能使模型处理上下文扩展到百万级别的方法，现在微软又提出了一种能扩展到十亿级别的方法（不过有标题党的嫌疑，因为在实验中作者只扩展到了百万级别）</p>
]]></summary>
        <content type="html"><![CDATA[<p>前段时间刚介绍了能使模型处理上下文扩展到百万级别的方法，现在微软又提出了一种能扩展到十亿级别的方法（不过有标题党的嫌疑，因为在实验中作者只扩展到了百万级别）</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Transformer to 1M tokens and beyond with RMT]]></title>
        <id>https://voluntexi.github.io/scalingTo1m/</id>
        <link href="https://voluntexi.github.io/scalingTo1m/">
        </link>
        <updated>2023-07-07T14:16:06.000Z</updated>
        <summary type="html"><![CDATA[<p>当我还在用最大一次只能处理1024个上下文的BART模型做实验时，已经有能处理上百万上下文的方法了🤡</p>
]]></summary>
        <content type="html"><![CDATA[<p>当我还在用最大一次只能处理1024个上下文的BART模型做实验时，已经有能处理上百万上下文的方法了🤡</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoRA]]></title>
        <id>https://voluntexi.github.io/lora/</id>
        <link href="https://voluntexi.github.io/lora/">
        </link>
        <updated>2023-06-04T11:05:35.000Z</updated>
        <summary type="html"><![CDATA[<p>在如今大模型时代，如果需要微调一个大模型无疑在时间和金钱方面的消耗是巨大的，而LoRA通过冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层中，大大减少了下游任务的可训练参数的数量。尽管LoRA使得可训练参数更少，但是与微调效果相比结果相当甚至更好。</p>
]]></summary>
        <content type="html"><![CDATA[<p>在如今大模型时代，如果需要微调一个大模型无疑在时间和金钱方面的消耗是巨大的，而LoRA通过冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层中，大大减少了下游任务的可训练参数的数量。尽管LoRA使得可训练参数更少，但是与微调效果相比结果相当甚至更好。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longformer]]></title>
        <id>https://voluntexi.github.io/longformer/</id>
        <link href="https://voluntexi.github.io/longformer/">
        </link>
        <updated>2023-05-18T03:12:48.000Z</updated>
        <summary type="html"><![CDATA[<p>Longformer是一种用来拓展模型在长序列建模的能力算法，它提出了一种时空复杂度同文本序列长度呈线性关系的Self-Attention，用以保证能够使得模型高效处理长文本。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Longformer是一种用来拓展模型在长序列建模的能力算法，它提出了一种时空复杂度同文本序列长度呈线性关系的Self-Attention，用以保证能够使得模型高效处理长文本。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimCSE]]></title>
        <id>https://voluntexi.github.io/SimCSE/</id>
        <link href="https://voluntexi.github.io/SimCSE/">
        </link>
        <updated>2023-04-27T11:14:07.000Z</updated>
        <summary type="html"><![CDATA[<p>最近做实验需要用到Sentence  Embeddings（句向量），特地研究了一下句向量相关模型算法，其中 SimCSE 模型是目前比较火、效果也比较好的一个模型。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近做实验需要用到Sentence  Embeddings（句向量），特地研究了一下句向量相关模型算法，其中 SimCSE 模型是目前比较火、效果也比较好的一个模型。</p>
]]></content>
    </entry>
</feed>